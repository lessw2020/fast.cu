#pragma once
#include "matmul_utils.cuh"

namespace M7 {

// using barrier = cuda::barrier<cuda::thread_scope_block>;
namespace cde = cuda::device::experimental;
using namespace matmul_utils;

template <int BlockMajorSize, int BlockMinorSize>
__host__ static inline CUtensorMap
create_tensor_map(bf16 *gmem_ptr, int global_height, int global_width) {
  CUtensorMap tma_map;
  void *gmem_address = (void *)gmem_ptr;
  static_assert(BlockMinorSize >= 64);
  assert(global_width % 64 == 0);
  uint64_t gmem_prob_shape[5] = {64, (uint64_t)global_height,
                                 (uint64_t)global_width / 64, 1, 1};
  uint64_t gmem_prob_stride[5] = {sizeof(bf16) * global_width,
                                  64 * sizeof(bf16), 0, 0, 0};
  uint32_t smem_box_shape[5] = {64, uint32_t(BlockMajorSize),
                                uint32_t(BlockMinorSize / 64), 1, 1};
  uint32_t smem_box_stride[5] = {1, 1, 1, 1, 1};

  CUresult result = cuTensorMapEncodeTiled(
      &tma_map, CU_TENSOR_MAP_DATA_TYPE_BFLOAT16, 5, gmem_address,
      gmem_prob_shape, gmem_prob_stride, smem_box_shape, smem_box_stride,
      CU_TENSOR_MAP_INTERLEAVE_NONE, CU_TENSOR_MAP_SWIZZLE_128B,
      CU_TENSOR_MAP_L2_PROMOTION_NONE, CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE);

  assert(result == CUDA_SUCCESS);
  return tma_map;
}

CUtensorMap d_tma_map_A;
CUtensorMap d_tma_map_B;
int _prev_m = 0, _prev_n = 0, _prev_k = 0;

template <int WGMMA_N, int ScaleD, int ScaleA, int ScaleB, int TransA,
          int TransB>
__device__ __forceinline__ void wgmma(float d[WGMMA_N / 16][8], bf16 *sA,
                                      bf16 *sB) {
  static_assert(WGMMA_N == 32 || WGMMA_N == 64 || WGMMA_N == 128 ||
                WGMMA_N == 192 || WGMMA_N == 208 || WGMMA_N == 256);
  if constexpr (WGMMA_N == 256)
    wgmma256<ScaleD, ScaleA, ScaleB, TransA, TransB>(d, sA, sB);
  if constexpr (WGMMA_N == 192)
    wgmma192<ScaleD, ScaleA, ScaleB, TransA, TransB>(d, sA, sB);
  if constexpr (WGMMA_N == 128)
    wgmma128<ScaleD, ScaleA, ScaleB, TransA, TransB>(d, sA, sB);
  if constexpr (WGMMA_N == 64)
    wgmma64<ScaleD, ScaleA, ScaleB, TransA, TransB>(d, sA, sB);
  if constexpr (WGMMA_N == 32)
    wgmma32<ScaleD, ScaleA, ScaleB, TransA, TransB>(d, sA, sB);
}

template <int BM, int BN, int BK, int QSIZE> struct SMem {
  alignas(128) bf16 A[BM * BK * QSIZE];
  alignas(128) bf16 B[BK * BN * QSIZE];
};

template <uint32_t RegCount> __device__ void warpgroup_reg_alloc() {
  asm volatile("setmaxnreg.inc.sync.aligned.u32 %0;\n" : : "n"(RegCount));
}

template <uint32_t RegCount> __device__ void warpgroup_reg_dealloc() {
  asm volatile("setmaxnreg.dec.sync.aligned.u32 %0;\n" : : "n"(RegCount));
}

__device__ static __forceinline__ void
init_barrier(uint64_t *bar, int thread_count, int transaction_count) {
  uint32_t bar_ptr = static_cast<uint32_t>(__cvta_generic_to_shared(bar));
  asm volatile("mbarrier.init.shared::cta.b64 [%0], %1;\n" ::"r"(bar_ptr),
               "r"(thread_count + transaction_count));
}

__device__ static __forceinline__ void expect_bytes(uint64_t *bar,
                                                    uint32_t bytes) {
  uint32_t bar_ptr = static_cast<uint32_t>(__cvta_generic_to_shared(bar));
  asm volatile(
      "mbarrier.arrive.expect_tx.shared::cta.b64 _, [%0], %1;\n" ::"r"(bar_ptr),
      "r"(bytes));
}

__device__ static inline void load_async(bf16 *dst,
                                         void const *const src_tma_map,
                                         uint64_t *bar, int global_col_idx,
                                         int global_row_idx) {
  uint64_t tma_ptr = reinterpret_cast<uint64_t>(src_tma_map);
  uint32_t mbar_ptr = static_cast<uint32_t>(__cvta_generic_to_shared(bar));
  uint32_t dst_ptr = static_cast<uint32_t>(__cvta_generic_to_shared(dst));

  asm volatile("cp.async.bulk.tensor.5d.shared::cluster.global.tile.mbarrier::"
               "complete_tx::bytes"
               " [%0], [%1, {%3, %4, %5, 0, 0}], [%2];"
               :
               : "r"(dst_ptr), "l"(tma_ptr), "r"(mbar_ptr), "n"(0),
                 "r"(global_row_idx), "r"(global_col_idx / 64)
               : "memory");
}

template <int VERSION, int NUM_SM, int BM, int BN, int TM, int TN>
struct Schedule;

template <int NUM_SM, int BM, int BN, int TM, int TN>
struct Schedule<1, NUM_SM, BM, BN, TM, TN> {
  int block;
  int it;
  int total_blocks_m, total_blocks_n;

  __device__ __forceinline__ Schedule(int M, int N, int _block) {
    block = _block;
    it = 0;
    total_blocks_m = CEIL_DIV(M, BM);
    total_blocks_n = CEIL_DIV(N, BN);
    assert(CEIL_DIV(M, BM) % TM == 0 && total_blocks_n % TN == 0);
  }

  __device__ __forceinline__ bool next(int &block_m, int &block_n) {
    int num = it * NUM_SM + block;
    if (num >= total_blocks_m * total_blocks_n) {
      return false;
    }

    int cur_tile = num / (TM * TN);
    int cur_tile_pos = num % (TM * TN);
    block_m = TM * (cur_tile / (total_blocks_n / TN));
    block_n = TN * (cur_tile % (total_blocks_n / TN));
    block_m += cur_tile_pos / TN;
    block_n += cur_tile_pos % TN;
    ++it;
    return true;
  }
};

template <int BM, int BN, int BK, int NUM_THREADS, int QSIZE, int NUM_SM>
__global__ __launch_bounds__(NUM_THREADS) void matmulKernel7(
    int M, int N, int K, bf16 *C,
    const __grid_constant__ CUtensorMap tensorMapA,
    const __grid_constant__ CUtensorMap tensorMapB) {
  constexpr int WGMMA_M = 64, WGMMA_K = 16, WGMMA_N = BN;
  constexpr int num_consumers = (NUM_THREADS / 128) - 1;
  constexpr int B_WG_M = BM / num_consumers;

  extern __shared__ __align__(128) uint8_t smem[];
  SMem<BM, BN, BK, QSIZE> &s =
      *reinterpret_cast<SMem<BM, BN, BK, QSIZE> *>(smem);
  bf16 *sA = s.A;
  bf16 *sB = s.B;
  // Declare barriers
  __shared__ __align__(8) uint64_t full[QSIZE], empty[QSIZE];

  const int num_blocks_k = K / BK;
  int wg_idx = threadIdx.x / 128;
  int tid = threadIdx.x % 128;

  if (threadIdx.x == 0) {
    for (int i = 0; i < QSIZE; ++i) {
      init_barrier(&full[i], 0, 1);
      init_barrier(&empty[i], 0, num_consumers);
    }
  }
  __syncthreads();

  Schedule<1, NUM_SM, BM, BN, 16, 8> schedule(M, N, blockIdx.x);

  // Producer
  if (wg_idx == 0) {
    constexpr int num_regs = (num_consumers <= 2 ? 24 : 32);
    warpgroup_reg_dealloc<num_regs>();
    if (tid == 0) {
      int p = 0;
      int qidx = 0;
      int num_block_m, num_block_n;
      while (schedule.next(num_block_m, num_block_n)) {
        for (int block_k_iter = 0; block_k_iter < num_blocks_k;
             ++block_k_iter, ++qidx) {
          if (qidx == QSIZE) {
            qidx = 0;
            p ^= 1;
          }
          ptx_wait(&empty[qidx], p);
          expect_bytes(&full[qidx], (BK * BN + BK * BM) * sizeof(bf16));
          load_async(&sA[qidx * BK * BM], &tensorMapA, &full[qidx],
                     block_k_iter * BK, num_block_m * BM);
          load_async(&sB[qidx * BK * BN], &tensorMapB, &full[qidx],
                     block_k_iter * BK, num_block_n * BN);
        }
      }
    }
  } else {
    constexpr int num_regs =
        (num_consumers == 1 ? 256 : (num_consumers == 2 ? 240 : 160));
    warpgroup_reg_alloc<num_regs>();
    float d[B_WG_M / WGMMA_M][WGMMA_N / 16][8];
    --wg_idx;
    for (int i = 0; i < QSIZE; ++i) {
      if (tid == 0)
        ptx_arrive(&empty[i], 1);
    }
    int p = 0;
    int qidx = 0;
    int num_block_m, num_block_n;
    while (schedule.next(num_block_m, num_block_n)) {
      memset(d, 0, sizeof(d));
      for (int block_k_iter = 0; block_k_iter < num_blocks_k;
           ++block_k_iter, ++qidx) {
        if (qidx == QSIZE) {
          qidx = 0;
          p ^= 1;
        };
        ptx_wait(&full[qidx], p);
        warpgroup_arrive();
#pragma unroll
        for (int m_it = 0; m_it < B_WG_M / WGMMA_M; ++m_it) {
          bf16 *wgmma_sA = sA + qidx * BK * BM +
                           64 * (m_it + wg_idx * B_WG_M / WGMMA_M) * WGMMA_M;
          bf16 *wgmma_sB = sB + qidx * BK * BN;
#pragma unroll
          for (int bk = 0; bk < BK; bk += 64) {
#pragma unroll
            for (int k_it = 0; k_it < 64 / WGMMA_K; ++k_it) {
              wgmma<WGMMA_N, 1, 1, 1, 0, 0>(d[m_it], &wgmma_sA[k_it * WGMMA_K],
                                            &wgmma_sB[k_it * WGMMA_K]);
            }
            wgmma_sA += 64 * BM;
            wgmma_sB += 64 * BN;
          }
        }
        warpgroup_commit_batch();
        warpgroup_wait<0>();
        if (tid == 0)
          ptx_arrive(&empty[qidx], 1);
      }

      int lane = tid % 32, warp = tid / 32, row = warp * 16 + lane / 4;
      bf16 *block_C = C + num_block_n * BN * M + num_block_m * BM;

#pragma unroll
      for (int m_it = 0; m_it < B_WG_M / WGMMA_M; ++m_it) {
        int yo = m_it * WGMMA_M + wg_idx * B_WG_M;
        if (row + yo + num_block_m * BM >= M)
          continue;
#pragma unroll
        for (int w = 0; w < WGMMA_N; w += 16)
          if (w < N - num_block_n * BN) {
            int col = w + 2 * (tid % 4);
#define IDX(i, j) ((j) * M + ((i) + yo))
#define ST(i, j, v) block_C[IDX(i, j)] = v;

            ST(row, col, d[m_it][w / 16][0]);
            ST(row, col + 1, d[m_it][w / 16][1]);
            ST(row + 8, col, d[m_it][w / 16][2]);
            ST(row + 8, col + 1, d[m_it][w / 16][3]);
            ST(row, col + 8, d[m_it][w / 16][4]);
            ST(row, col + 9, d[m_it][w / 16][5]);
            ST(row + 8, col + 8, d[m_it][w / 16][6]);
            ST(row + 8, col + 9, d[m_it][w / 16][7]);
#undef IDX
#undef ST
          }
      }
    }
  }
}

void runKernel7(int M, int N, int K, bf16 *A, bf16 *B, bf16 *C, int *DB) {
  constexpr int BM = 128;
  constexpr int BN = 256;
  constexpr int BK = 64;
  constexpr int NUM_THREADS = 128 * 3;
  constexpr int QSIZE = 3;
  constexpr int NUM_SM = 128;

  if (_prev_m != M) {
    d_tma_map_A = create_tensor_map<BM, BK>(A, M, K);
    d_tma_map_B = create_tensor_map<BN, BK>(B, N, K);
    _prev_m = M;
    _prev_n = N;
    _prev_k = K;
  }
  // Assert cached values are of same size
  assert(M == _prev_m && N == _prev_n && K == _prev_k);
  auto *kernel = matmulKernel7<BM, BN, BK, NUM_THREADS, QSIZE, NUM_SM>;
  size_t sMemSize = sizeof(SMem<BM, BN, BK, QSIZE>);
  cudaCheck(cudaFuncSetAttribute(
      kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, sMemSize));

  kernel<<<NUM_SM, NUM_THREADS, sMemSize>>>(M, N, K, C, d_tma_map_A,
                                            d_tma_map_B);
}

} // namespace M7

using M7::runKernel7;
